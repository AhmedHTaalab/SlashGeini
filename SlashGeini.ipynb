{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzmF9IBoDSOuqOVOlXSFmA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedHTaalab/SlashGeini/blob/main/SlashGeini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RTj6WyROtaXn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50eaed3f-33b1-4fc8-ee0f-2f6c26b9ae1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.0/165.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.4/725.4 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m venv venv\n",
        "!pip3 install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wObECu9t09f",
        "outputId": "eaf48350-b3cc-4499-ce3d-87fb978734c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The virtual environment was not created successfully because ensurepip is not\n",
            "available.  On Debian/Ubuntu systems, you need to install the python3-venv\n",
            "package using the following command.\n",
            "\n",
            "    apt install python3.10-venv\n",
            "\n",
            "You may need to use sudo with that command.  After installing the python3-venv\n",
            "package, recreate your virtual environment.\n",
            "\n",
            "Failing command: /content/venv/bin/python3\n",
            "\n",
            "Collecting streamlit~=1.38.0 (from -r requirements.txt (line 1))\n",
            "  Downloading streamlit-1.38.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting pandas~=2.0.3 (from -r requirements.txt (line 2))\n",
            "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting nltk~=3.9.1 (from -r requirements.txt (line 3))\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: google.generativeai in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.8.1)\n",
            "Collecting watchdog (from -r requirements.txt (line 5))\n",
            "  Downloading watchdog-5.0.2-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit~=1.38.0->-r requirements.txt (line 1)) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit~=1.38.0->-r requirements.txt (line 1)) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit~=1.38.0->-r requirements.txt (line 1)) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit~=1.38.0->-r requirements.txt (line 1)) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit~=1.38.0->-r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit~=1.38.0->-r requirements.txt (line 1)) (24.1)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit~=1.38.0->-r requirements.txt (line 1)) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit~=1.38.0->-r requirements.txt (line 1)) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit~=1.38.0->-r requirements.txt (line 1)) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit~=1.38.0->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit~=1.38.0->-r requirements.txt (line 1)) (13.8.1)\n",
            "Collecting tenacity<9,>=8.1.0 (from streamlit~=1.38.0->-r requirements.txt (line 1))\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit~=1.38.0->-r requirements.txt (line 1)) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit~=1.38.0->-r requirements.txt (line 1)) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit~=1.38.0->-r requirements.txt (line 1))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit~=1.38.0->-r requirements.txt (line 1))\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit~=1.38.0->-r requirements.txt (line 1)) (6.3.3)\n",
            "Collecting watchdog (from -r requirements.txt (line 5))\n",
            "  Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl.metadata (38 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas~=2.0.3->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=2.0.3->-r requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=2.0.3->-r requirements.txt (line 2)) (2024.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk~=3.9.1->-r requirements.txt (line 3)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk~=3.9.1->-r requirements.txt (line 3)) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk~=3.9.1->-r requirements.txt (line 3)) (4.66.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.9 in /usr/local/lib/python3.10/dist-packages (from google.generativeai->-r requirements.txt (line 4)) (0.6.9)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google.generativeai->-r requirements.txt (line 4)) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google.generativeai->-r requirements.txt (line 4)) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google.generativeai->-r requirements.txt (line 4)) (2.27.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google.generativeai->-r requirements.txt (line 4)) (2.9.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.9->google.generativeai->-r requirements.txt (line 4)) (1.24.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit~=1.38.0->-r requirements.txt (line 1)) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit~=1.38.0->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit~=1.38.0->-r requirements.txt (line 1)) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit~=1.38.0->-r requirements.txt (line 1)) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit~=1.38.0->-r requirements.txt (line 1))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google.generativeai->-r requirements.txt (line 4)) (1.65.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google.generativeai->-r requirements.txt (line 4)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google.generativeai->-r requirements.txt (line 4)) (4.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas~=2.0.3->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit~=1.38.0->-r requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit~=1.38.0->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit~=1.38.0->-r requirements.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit~=1.38.0->-r requirements.txt (line 1)) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit~=1.38.0->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit~=1.38.0->-r requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google.generativeai->-r requirements.txt (line 4)) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google.generativeai->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google.generativeai->-r requirements.txt (line 4)) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google.generativeai->-r requirements.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->google.generativeai->-r requirements.txt (line 4)) (2.23.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit~=1.38.0->-r requirements.txt (line 1))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.9->google.generativeai->-r requirements.txt (line 4)) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.9->google.generativeai->-r requirements.txt (line 4)) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google.generativeai->-r requirements.txt (line 4)) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit~=1.38.0->-r requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.38.0->-r requirements.txt (line 1)) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.38.0->-r requirements.txt (line 1)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.38.0->-r requirements.txt (line 1)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.38.0->-r requirements.txt (line 1)) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit~=1.38.0->-r requirements.txt (line 1)) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google.generativeai->-r requirements.txt (line 4)) (0.6.1)\n",
            "Downloading streamlit-1.38.0-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: watchdog, tenacity, smmap, nltk, pydeck, pandas, gitdb, gitpython, streamlit\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.1.4\n",
            "    Uninstalling pandas-2.1.4:\n",
            "      Successfully uninstalled pandas-2.1.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.1.4, but you have pandas 2.0.3 which is incompatible.\n",
            "mizani 0.11.4 requires pandas>=2.1.0, but you have pandas 2.0.3 which is incompatible.\n",
            "plotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 2.0.3 which is incompatible.\n",
            "xarray 2024.9.0 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gitdb-4.0.11 gitpython-3.1.43 nltk-3.9.1 pandas-2.0.3 pydeck-0.9.1 smmap-5.0.1 streamlit-1.38.0 tenacity-8.5.0 watchdog-4.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKLv-Vmzur_U",
        "outputId": "589e773b-d96b-4a2d-845f-83e9c92ada62"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.38.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.8.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pandas google-generativeai nltk\n",
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5psRhJF1y-di",
        "outputId": "34343225-715f-40f3-aedd-15b734d348d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.38.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.8.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.9 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.9)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.9->google-generativeai) (1.24.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.65.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.23.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.9->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.9->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHa26_e8ESvX",
        "outputId": "c34f3d8d-d0f0-41df-b006-addc491c52c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile app.py\n",
        "# import streamlit as st\n",
        "# import pandas as pd\n",
        "# import google.generativeai as genai\n",
        "# import os\n",
        "# import json\n",
        "# import re\n",
        "\n",
        "# # Set your API key here directly\n",
        "# os.environ[\"API_KEY\"] = \"AIzaSyAFHyRhWWEVGTzNXH3xHq8vBx229DzVkPM\"\n",
        "# genai.configure(api_key=os.environ[\"API_KEY\"])\n",
        "# model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "# # Load schema for Gemini model (if needed for your specific summarization task)\n",
        "# with open(\"./scheme.json\", \"r\") as f:\n",
        "#     gemini_flash_schema = json.load(f)\n",
        "\n",
        "# # Preprocess text function\n",
        "# def preprocess_text(text):\n",
        "#     stopwords = {\n",
        "#         \"the\", \"is\", \"in\", \"at\", \"on\", \"a\", \"an\", \"and\", \"or\", \"for\", \"to\", \"of\", \"with\", \"that\", \"by\", \"it\",\n",
        "#     }\n",
        "#     text = re.sub(r\"\\d+|[^\\w\\s]|\\s+\", \" \", text.lower()).strip()\n",
        "#     return \" \".join([word for word in text.split() if word not in stopwords])\n",
        "\n",
        "# # Generate sentiment and grade using Gemini\n",
        "# def generate_review_grade_with_sentiment(review_text):\n",
        "#     try:\n",
        "#         prompt = f\"\"\"\n",
        "#         Analyze the following review: {review_text}.\n",
        "\n",
        "#         Determine its sentiment (positive, neutral, or negative) based on your analysis. You can use these examples as a reference, but the actual sentiment should be based on the review's content:\n",
        "#         - **Positive**: \"The product was exactly as described, high quality, and arrived quickly.\" (Example grade: 4 or 5)\n",
        "#         - **Neutral**: \"The product is okay, nothing special, but it works as expected.\" (Example grade: 3)\n",
        "#         - **Negative**: \"The product was poorly made, broke easily, and did not meet expectations.\" (Example grade: 1 or 2)\n",
        "\n",
        "#         After analyzing the review, assign a grade from 1 to 5:\n",
        "#         - **4 or 5** for positive reviews.\n",
        "#         - **3** for neutral reviews.\n",
        "#         - **1 or 2** for negative reviews.\n",
        "\n",
        "#         Make sure the grade reflects the overall tone and content of the review.\n",
        "#         \"\"\"\n",
        "#         response = model.generate_content(prompt)\n",
        "\n",
        "#         st.write(f\"API Response: {response.text}\")  # For debugging\n",
        "\n",
        "#         sentiment_match = re.search(r\"(positive|negative|neutral)\", response.text, re.IGNORECASE)\n",
        "#         grade_match = re.search(r\"\\d(\\.\\d+)?\", response.text)\n",
        "\n",
        "#         if sentiment_match and grade_match:\n",
        "#             sentiment_label = sentiment_match.group().upper()\n",
        "#             grade = float(grade_match.group())\n",
        "#             return sentiment_label, grade\n",
        "#         else:\n",
        "#             st.write(f\"No valid sentiment or grade found in response: {response.text}\")\n",
        "#             return None, None\n",
        "#     except Exception as e:\n",
        "#         st.error(f\"Error generating sentiment and grade for review: {e}\")\n",
        "#         return None, None\n",
        "\n",
        "# # Generate summary using Gemini\n",
        "# def generate_summary(text):\n",
        "#     try:\n",
        "#         schema_str = json.dumps(gemini_flash_schema)\n",
        "#         prompt = f\"Using the following constraints: {schema_str}, summarize the following text: {text}\"\n",
        "#         response = model.generate_content(prompt)\n",
        "#         summary = response.text.strip()\n",
        "#         return summary\n",
        "#     except Exception as e:\n",
        "#         st.error(f\"Error generating summary: {e}\")\n",
        "#         return \"Summary could not be generated.\"\n",
        "\n",
        "# # Generate pros and cons using Gemini\n",
        "# def generate_pros_and_cons(text):\n",
        "#     try:\n",
        "#         schema_str = json.dumps(gemini_flash_schema)\n",
        "#         prompt = f\"Using the following constraints: {schema_str}, extract pros and cons from the following text: {text}\"\n",
        "#         response = model.generate_content(prompt)\n",
        "#         response_text = response.text.strip()\n",
        "\n",
        "#         pros, cons = \"\", \"\"\n",
        "#         if \"Pros:\" in response_text:\n",
        "#             pros = response_text.split(\"Pros:\")[1].split(\"Cons:\")[0].strip()\n",
        "#         if \"Cons:\" in response_text:\n",
        "#             cons = response_text.split(\"Cons:\")[1].strip()\n",
        "\n",
        "#         return pros, cons\n",
        "#     except Exception as e:\n",
        "#         st.error(f\"Error generating pros and cons: {e}\")\n",
        "#         return \"Pros could not be generated.\", \"Cons could not be generated.\"\n",
        "\n",
        "# # Calculate mean grades\n",
        "# def calculate_mean_grades():\n",
        "#     encodings = [\"latin1\", \"ISO-8859-1\", \"cp1252\"]\n",
        "#     for enc in encodings:\n",
        "#         try:\n",
        "#             df = pd.read_csv(\"/content/English_merged_outputv4.csv\", encoding=enc)\n",
        "#             global_avg_rating = df[\"product_rating\"].mean()\n",
        "#             min_raters = 35\n",
        "#             result = {}\n",
        "\n",
        "#             for product in df[\"product_name\"].unique():\n",
        "#                 filtered_reviews = df[df[\"product_name\"] == product]\n",
        "#                 result[product] = process_product_reviews(filtered_reviews, global_avg_rating, min_raters)\n",
        "\n",
        "#             st.write(\"Product Grades:\")\n",
        "#             st.json(result)\n",
        "#             return result\n",
        "\n",
        "#         except UnicodeDecodeError as e:\n",
        "#             st.error(f\"Error: {e}\")\n",
        "#             continue\n",
        "\n",
        "# # Process product reviews\n",
        "# def process_product_reviews(filtered_reviews, global_avg_rating, min_raters):\n",
        "#     if filtered_reviews.empty:\n",
        "#         return {\n",
        "#             \"mean_grade\": None,\n",
        "#             \"final_rate\": None,\n",
        "#             \"grades\": [],\n",
        "#         }\n",
        "\n",
        "#     grades, total_weighted_rating, total_mean_grade = [], 0, 0\n",
        "#     for _, row in filtered_reviews.iterrows():\n",
        "#         review_text = preprocess_text(row[\"product_review_name\"])\n",
        "#         sentiment_label, grade = generate_review_grade_with_sentiment(review_text)\n",
        "\n",
        "#         if grade is not None:\n",
        "#             grades.append(grade)\n",
        "#             st.write(f\"Review: {row['product_review_name']}, Sentiment: {sentiment_label}, Grade: {grade}\")\n",
        "\n",
        "#             weighted_rating = (\n",
        "#                 (row[\"product_rating\"] * row[\"product_number_of_rating\"])\n",
        "#                 + (global_avg_rating * min_raters)\n",
        "#             ) / (row[\"product_number_of_rating\"] + min_raters)\n",
        "#             total_weighted_rating += weighted_rating\n",
        "#             total_mean_grade += grade\n",
        "\n",
        "#     if grades:\n",
        "#         mean_grade = sum(grades) / len(grades)\n",
        "#         final_rate = (total_mean_grade / len(grades) + total_weighted_rating / len(filtered_reviews)) / 2\n",
        "#         return {\n",
        "#             \"mean_grade\": mean_grade,\n",
        "#             \"final_rate\": final_rate,\n",
        "#             \"grades\": grades,\n",
        "#         }\n",
        "#     else:\n",
        "#         return {\n",
        "#             \"mean_grade\": None,\n",
        "#             \"final_rate\": None,\n",
        "#             \"grades\": [],\n",
        "#         }\n",
        "\n",
        "# # Streamlit App Layout\n",
        "# st.title(\"Product Review Analyzer and Grader\")\n",
        "\n",
        "# # Input product name for summarization, pros/cons extraction, and grading\n",
        "# product_name = st.text_input(\"Enter Product Name:\")\n",
        "\n",
        "# if product_name:\n",
        "#     default_encoding = \"latin1\"\n",
        "#     st.subheader(f\"Reviews for {product_name}\")\n",
        "#     try:\n",
        "#         df = pd.read_csv(\"/content/English_merged_outputv4.csv\", encoding=default_encoding)\n",
        "#     except UnicodeDecodeError as e:\n",
        "#         st.error(f\"Error reading file: {e}\")\n",
        "\n",
        "#     filtered_reviews = df[df[\"product_name\"].str.contains(product_name, case=False)]\n",
        "\n",
        "#     if not filtered_reviews.empty:\n",
        "#         combined_reviews_text = \" \".join(filtered_reviews[\"product_review_name\"].tolist())\n",
        "\n",
        "#         # Summarize reviews\n",
        "#         st.subheader(\"Summarization\")\n",
        "#         summary = generate_summary(combined_reviews_text)\n",
        "#         st.write(f\"Summary:\\n{summary}\")\n",
        "\n",
        "#         # Generate pros and cons\n",
        "#         st.subheader(\"Pros and Cons\")\n",
        "#         pros, cons = generate_pros_and_cons(combined_reviews_text)\n",
        "#         st.write(f\"**Pros:**\\n{pros}\")\n",
        "#         st.write(f\"**Cons:**\\n{cons}\")\n",
        "\n",
        "#         # Calculate grades\n",
        "#         st.subheader(\"Grades and Ratings\")\n",
        "#         result = process_product_reviews(filtered_reviews, df[\"product_rating\"].mean(), 35)\n",
        "#         st.json(result)\n",
        "\n",
        "#     else:\n",
        "#         st.error(f\"No reviews found for product: {product_name}\")\n",
        "\n",
        "\n",
        "\n",
        "# %%writefile app.py\n",
        "# import streamlit as st\n",
        "# import pandas as pd\n",
        "# import google.generativeai as genai\n",
        "# import os\n",
        "# import json\n",
        "# import re\n",
        "\n",
        "# # Set your API key here directly\n",
        "# os.environ[\"API_KEY\"] = \"AIzaSyAFHyRhWWEVGTzNXH3xHq8vBx229DzVkPM\"\n",
        "# genai.configure(api_key=os.environ[\"API_KEY\"])\n",
        "# model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "# # Load schema for Gemini model (if needed for your specific summarization task)\n",
        "# with open(\"./scheme.json\", \"r\") as f:\n",
        "#     gemini_flash_schema = json.load(f)\n",
        "\n",
        "# # Preprocess text function\n",
        "# def preprocess_text(text):\n",
        "#     stopwords = {\n",
        "#         \"the\", \"is\", \"in\", \"at\", \"on\", \"a\", \"an\", \"and\", \"or\", \"for\", \"to\", \"of\", \"with\", \"that\", \"by\", \"it\",\n",
        "#     }\n",
        "#     text = re.sub(r\"\\d+|[^\\w\\s]|\\s+\", \" \", text.lower()).strip()\n",
        "#     return \" \".join([word for word in text.split() if word not in stopwords])\n",
        "\n",
        "# # Generate sentiment and grade using Gemini\n",
        "# def generate_review_grade_with_sentiment(review_text):\n",
        "#     try:\n",
        "#         prompt = f\"\"\"\n",
        "#         Analyze the following review: {review_text}.\n",
        "\n",
        "#         Determine its sentiment (positive, neutral, or negative) based on your analysis. You can use these examples as a reference, but the actual sentiment should be based on the review's content:\n",
        "#         - **Positive**: \"The product was exactly as described, high quality, and arrived quickly.\" (Example grade: 4 or 5)\n",
        "#         - **Neutral**: \"The product is okay, nothing special, but it works as expected.\" (Example grade: 3)\n",
        "#         - **Negative**: \"The product was poorly made, broke easily, and did not meet expectations.\" (Example grade: 1 or 2)\n",
        "\n",
        "#         After analyzing the review, assign a grade from 1 to 5:\n",
        "#         - **4 or 5** for positive reviews.\n",
        "#         - **3** for neutral reviews.\n",
        "#         - **1 or 2** for negative reviews.\n",
        "\n",
        "#         Make sure the grade reflects the overall tone and content of the review.\n",
        "#         \"\"\"\n",
        "#         response = model.generate_content(prompt)\n",
        "\n",
        "#         st.write(f\"API Response: {response.text}\")  # For debugging\n",
        "\n",
        "#         sentiment_match = re.search(r\"(positive|negative|neutral)\", response.text, re.IGNORECASE)\n",
        "#         grade_match = re.search(r\"\\d(\\.\\d+)?\", response.text)\n",
        "\n",
        "#         if sentiment_match and grade_match:\n",
        "#             sentiment_label = sentiment_match.group().upper()\n",
        "#             grade = float(grade_match.group())\n",
        "#             return sentiment_label, grade\n",
        "#         else:\n",
        "#             st.write(f\"No valid sentiment or grade found in response: {response.text}\")\n",
        "#             return None, None\n",
        "#     except Exception as e:\n",
        "#         st.error(f\"Error generating sentiment and grade for review: {e}\")\n",
        "#         return None, None\n",
        "\n",
        "# # Generate summary using Gemini\n",
        "# def generate_summary(text):\n",
        "#     try:\n",
        "#         schema_str = json.dumps(gemini_flash_schema)\n",
        "#         prompt = f\"Using the following constraints: {schema_str}, summarize the following text: {text}\"\n",
        "#         response = model.generate_content(prompt)\n",
        "#         summary = response.text.strip()\n",
        "#         return summary\n",
        "#     except Exception as e:\n",
        "#         st.error(f\"Error generating summary: {e}\")\n",
        "#         return \"Summary could not be generated.\"\n",
        "\n",
        "# # Generate pros and cons using Gemini\n",
        "# def generate_pros_and_cons(text):\n",
        "#     try:\n",
        "#         schema_str = json.dumps(gemini_flash_schema)\n",
        "#         prompt = f\"Using the following constraints: {schema_str}, extract pros and cons from the following text: {text}\"\n",
        "#         response = model.generate_content(prompt)\n",
        "#         response_text = response.text.strip()\n",
        "\n",
        "#         pros, cons = \"\", \"\"\n",
        "#         if \"Pros:\" in response_text:\n",
        "#             pros = response_text.split(\"Pros:\")[1].split(\"Cons:\")[0].strip()\n",
        "#         if \"Cons:\" in response_text:\n",
        "#             cons = response_text.split(\"Cons:\")[1].strip()\n",
        "\n",
        "#         return pros, cons\n",
        "#     except Exception as e:\n",
        "#         st.error(f\"Error generating pros and cons: {e}\")\n",
        "#         return \"Pros could not be generated.\", \"Cons could not be generated.\"\n",
        "\n",
        "# # Calculate mean grades\n",
        "# def calculate_mean_grades():\n",
        "#     encodings = [\"latin1\", \"ISO-8859-1\", \"cp1252\"]\n",
        "#     for enc in encodings:\n",
        "#         try:\n",
        "#             df = pd.read_csv(\"/content/English_Reviews_WithNewDateISO&IDColumn-WhichIdon'tAgreeOn.csv\", encoding=enc)\n",
        "#             global_avg_rating = df[\"product_rating\"].mean()\n",
        "#             min_raters = 35\n",
        "#             result = {}\n",
        "\n",
        "#             for product in df[\"product_name\"].unique():\n",
        "#                 filtered_reviews = df[df[\"product_name\"] == product]\n",
        "#                 result[product] = process_product_reviews(filtered_reviews, global_avg_rating, min_raters)\n",
        "\n",
        "#             st.write(\"Product Grades:\")\n",
        "#             st.json(result)\n",
        "#             return result\n",
        "\n",
        "#         except UnicodeDecodeError as e:\n",
        "#             st.error(f\"Error: {e}\")\n",
        "#             continue\n",
        "\n",
        "# # Process product reviews\n",
        "# def process_product_reviews(filtered_reviews, global_avg_rating, min_raters):\n",
        "#     if filtered_reviews.empty:\n",
        "#         return {\n",
        "#             \"mean_grade\": None,\n",
        "#             \"final_rate\": None,\n",
        "#             \"grades\": [],\n",
        "#         }\n",
        "\n",
        "#     grades, total_weighted_rating, total_mean_grade = [], 0, 0\n",
        "#     for _, row in filtered_reviews.iterrows():\n",
        "#         review_text = preprocess_text(row[\"product_review_name\"])\n",
        "#         sentiment_label, grade = generate_review_grade_with_sentiment(review_text)\n",
        "\n",
        "#         if grade is not None:\n",
        "#             grades.append(grade)\n",
        "#             st.write(f\"Review: {row['product_review_name']}, Sentiment: {sentiment_label}, Grade: {grade}\")\n",
        "\n",
        "#             weighted_rating = (\n",
        "#                 (row[\"product_rating\"] * row[\"product_number_of_rating\"])\n",
        "#                 + (global_avg_rating * min_raters)\n",
        "#             ) / (row[\"product_number_of_rating\"] + min_raters)\n",
        "#             total_weighted_rating += weighted_rating\n",
        "#             total_mean_grade += grade\n",
        "\n",
        "#     if grades:\n",
        "#         mean_grade = sum(grades) / len(grades)\n",
        "#         final_rate = (total_mean_grade / len(grades) + total_weighted_rating / len(filtered_reviews)) / 2\n",
        "#         return {\n",
        "#             \"mean_grade\": mean_grade,\n",
        "#             \"final_rate\": final_rate,\n",
        "#             \"grades\": grades,\n",
        "#         }\n",
        "#     else:\n",
        "#         return {\n",
        "#             \"mean_grade\": None,\n",
        "#             \"final_rate\": None,\n",
        "#             \"grades\": [],\n",
        "#         }\n",
        "\n",
        "# # # Streamlit App Layout\n",
        "# # st.title(\"Product Review Analyzer and Grader\")\n",
        "\n",
        "# # # Input product name for summarization, pros/cons extraction, and grading\n",
        "# # product_name = st.text_input(\"Enter Product Name:\")\n",
        "\n",
        "# # if product_name:\n",
        "# #     default_encoding = \"latin1\"\n",
        "# #     st.subheader(f\"Reviews for {product_name}\")\n",
        "# #     try:\n",
        "# #         df = pd.read_csv(\"/content/English_merged_outputv4.csv\", encoding=default_encoding)\n",
        "# #     except UnicodeDecodeError as e:\n",
        "# #         st.error(f\"Error reading file: {e}\")\n",
        "\n",
        "# #     filtered_reviews = df[df[\"product_name\"].str.contains(product_name, case=False)]\n",
        "\n",
        "# #     if not filtered_reviews.empty:\n",
        "# #         combined_reviews_text = \" \".join(filtered_reviews[\"product_review_name\"].tolist())\n",
        "\n",
        "# #         # Summarize reviews\n",
        "# #         st.subheader(\"Summarization\")\n",
        "# #         summary = generate_summary(combined_reviews_text)\n",
        "# #         st.write(f\"Summary:\\n{summary}\")\n",
        "\n",
        "# #         # Generate pros and cons\n",
        "# #         st.subheader(\"Pros and Cons\")\n",
        "# #         pros, cons = generate_pros_and_cons(combined_reviews_text)\n",
        "# #         st.write(f\"**Pros:**\\n{pros}\")\n",
        "# #         st.write(f\"**Cons:**\\n{cons}\")\n",
        "\n",
        "# #         # Calculate grades\n",
        "# #         st.subheader(\"Grades and Ratings\")\n",
        "# #         result = process_product_reviews(filtered_reviews, df[\"product_rating\"].mean(), 35)\n",
        "# #         st.json(result)\n",
        "\n",
        "# #     else:\n",
        "# #         st.error(f\"No reviews found for product: {product_name}\")\n",
        "\n",
        "# # Streamlit App Layout\n",
        "# st.title(\"Product Review Analyzer and Grader\")\n",
        "\n",
        "# # Input product name for summarization, pros/cons extraction, and grading\n",
        "# product_name = st.text_input(\"Enter Product Name:\")\n",
        "\n",
        "# if product_name:\n",
        "#     default_encoding = \"latin1\"\n",
        "#     st.subheader(f\"Reviews for {product_name}\")\n",
        "#     try:\n",
        "#         df = pd.read_csv(\"/content/English_Reviews_WithNewDateISO&IDColumn-WhichIdon'tAgreeOn.csv\", encoding=default_encoding)\n",
        "#     except UnicodeDecodeError as e:\n",
        "#         st.error(f\"Error reading file: {e}\")\n",
        "\n",
        "#     filtered_reviews = df[df[\"product_name\"].str.contains(product_name, case=False)]\n",
        "\n",
        "#     if not filtered_reviews.empty:\n",
        "#         # Sort by Date and get the latest 5 reviews\n",
        "#         filtered_reviews['Date'] = pd.to_datetime(filtered_reviews['Date'])\n",
        "#         latest_reviews = filtered_reviews.sort_values(by='Date', ascending=False).head(5)\n",
        "\n",
        "#         # st.subheader(\"Latest 5 Reviews\")\n",
        "#         # for index, row in latest_reviews.iterrows():\n",
        "#         #     st.write(f\"Review: {row['product_review_name']} (Date: {row['Date']})\")\n",
        "\n",
        "#         combined_reviews_text = \" \".join(filtered_reviews[\"product_review_name\"].tolist())\n",
        "\n",
        "#         # Summarize reviews\n",
        "#         st.subheader(\"Summarization\")\n",
        "#         summary = generate_summary(combined_reviews_text)\n",
        "#         st.write(f\"Summary:\\n{summary}\")\n",
        "\n",
        "#         # Generate pros and cons\n",
        "#         st.subheader(\"Pros and Cons\")\n",
        "#         pros, cons = generate_pros_and_cons(combined_reviews_text)\n",
        "#         st.write(f\"**Pros:**\\n{pros}\")\n",
        "#         st.write(f\"**Cons:**\\n{cons}\")\n",
        "\n",
        "#         # Calculate grades\n",
        "#         st.subheader(\"Grades and Ratings\")\n",
        "#         result = process_product_reviews(filtered_reviews, df[\"product_rating\"].mean(), 35)\n",
        "\n",
        "#         # Show all grades in JSON format\n",
        "#         all_grades_json = {\n",
        "#             \"product_name\": product_name,\n",
        "#             \"grades\": result[\"grades\"],\n",
        "#             \"mean_grade\": result[\"mean_grade\"],\n",
        "#             \"final_rate\": result[\"final_rate\"],\n",
        "#         }\n",
        "#         st.json(all_grades_json)\n",
        "\n",
        "#     else:\n",
        "#         st.error(f\"No reviews found for product: {product_name}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6vfRiCUnjkk",
        "outputId": "2feb6c16-443d-4e5a-ae62-6b9fd067ee81"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# %%writefile app.py\n",
        "# import streamlit as st\n",
        "# import pandas as pd\n",
        "# import google.generativeai as genai\n",
        "# import os\n",
        "# import json\n",
        "# import re\n",
        "\n",
        "# # Set your API key here directly\n",
        "# os.environ[\"API_KEY\"] = \"AIzaSyAFHyRhWWEVGTzNXH3xHq8vBx229DzVkPM\"\n",
        "# genai.configure(api_key=os.environ[\"API_KEY\"])\n",
        "# model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "# # Load schema for Gemini model (if needed for your specific summarization task)\n",
        "# with open(\"./scheme.json\", \"r\") as f:\n",
        "#     gemini_flash_schema = json.load(f)\n",
        "\n",
        "# # Preprocess text function\n",
        "# def preprocess_text(text):\n",
        "#     stopwords = {\n",
        "#         \"the\", \"is\", \"in\", \"at\", \"on\", \"a\", \"an\", \"and\", \"or\", \"for\", \"to\", \"of\", \"with\", \"that\", \"by\", \"it\",\n",
        "#     }\n",
        "#     text = re.sub(r\"\\d+|[^\\w\\s]|\\s+\", \" \", text.lower()).strip()\n",
        "#     return \" \".join([word for word in text.split() if word not in stopwords])\n",
        "\n",
        "# # Generate sentiment and grade using Gemini\n",
        "# def generate_review_grade_with_sentiment(review_text):\n",
        "#     try:\n",
        "#         prompt = f\"\"\"\n",
        "#         Analyze the following review: {review_text}.\n",
        "\n",
        "#         Determine its sentiment (positive, neutral, or negative) based on your analysis. You can use these examples as a reference, but the actual sentiment should be based on the review's content:\n",
        "#         - **Positive**: \"The product was exactly as described, high quality, and arrived quickly.\" (Example grade: 4 or 5)\n",
        "#         - **Neutral**: \"The product is okay, nothing special, but it works as expected.\" (Example grade: 3)\n",
        "#         - **Negative**: \"The product was poorly made, broke easily, and did not meet expectations.\" (Example grade: 1 or 2)\n",
        "\n",
        "#         After analyzing the review, assign a grade from 1 to 5:\n",
        "#         - **4 or 5** for positive reviews.\n",
        "#         - **3** for neutral reviews.\n",
        "#         - **1 or 2** for negative reviews.\n",
        "\n",
        "#         Make sure the grade reflects the overall tone and content of the review.\n",
        "#         \"\"\"\n",
        "#         response = model.generate_content(prompt)\n",
        "\n",
        "#         # Extract only sentiment and grade\n",
        "#         sentiment_match = re.search(r\"(positive|negative|neutral)\", response.text, re.IGNORECASE)\n",
        "#         grade_match = re.search(r\"\\d(\\.\\d+)?\", response.text)\n",
        "\n",
        "#         if sentiment_match and grade_match:\n",
        "#             sentiment_label = sentiment_match.group().upper()\n",
        "#             grade = float(grade_match.group())\n",
        "#             return sentiment_label, grade\n",
        "#         else:\n",
        "#             return \"Unknown\", None\n",
        "#     except Exception as e:\n",
        "#         st.error(f\"Error generating sentiment and grade for review: {e}\")\n",
        "#         return None, None\n",
        "\n",
        "# # Generate summary using Gemini\n",
        "# def generate_summary(text):\n",
        "#     try:\n",
        "#         schema_str = json.dumps(gemini_flash_schema)\n",
        "#         prompt = f\"Using the following constraints: {schema_str}, summarize the following text: {text}\"\n",
        "#         response = model.generate_content(prompt)\n",
        "#         summary = response.text.strip()\n",
        "#         return summary\n",
        "#     except Exception as e:\n",
        "#         st.error(f\"Error generating summary: {e}\")\n",
        "#         return \"Summary could not be generated.\"\n",
        "\n",
        "# # Generate pros and cons using Gemini\n",
        "# def generate_pros_and_cons(text):\n",
        "#     try:\n",
        "#         schema_str = json.dumps(gemini_flash_schema)\n",
        "#         prompt = f\"Using the following constraints: {schema_str}, extract pros and cons from the following text: {text}\"\n",
        "#         response = model.generate_content(prompt)\n",
        "#         response_text = response.text.strip()\n",
        "\n",
        "#         pros, cons = \"\", \"\"\n",
        "#         if \"Pros:\" in response_text:\n",
        "#             pros = response_text.split(\"Pros:\")[1].split(\"Cons:\")[0].strip()\n",
        "#         if \"Cons:\" in response_text:\n",
        "#             cons = response_text.split(\"Cons:\")[1].strip()\n",
        "\n",
        "#         return pros, cons\n",
        "#     except Exception as e:\n",
        "#         st.error(f\"Error generating pros and cons: {e}\")\n",
        "#         return \"Pros could not be generated.\", \"Cons could not be generated.\"\n",
        "\n",
        "# # Calculate mean grades\n",
        "# def calculate_mean_grades():\n",
        "#     encodings = [\"latin1\", \"ISO-8859-1\", \"cp1252\"]\n",
        "#     for enc in encodings:\n",
        "#         try:\n",
        "#             df = pd.read_csv(\"/content/English_Reviews_WithNewDateISO&IDColumn-WhichIdon'tAgreeOn.csv\", encoding=enc)\n",
        "#             global_avg_rating = df[\"product_rating\"].mean()\n",
        "#             min_raters = 35\n",
        "#             result = {}\n",
        "\n",
        "#             for product in df[\"product_name\"].unique():\n",
        "#                 filtered_reviews = df[df[\"product_name\"] == product]\n",
        "#                 result[product] = process_product_reviews(filtered_reviews, global_avg_rating, min_raters)\n",
        "\n",
        "#             st.write(\"Product Grades:\")\n",
        "#             st.json(result)\n",
        "#             return result\n",
        "\n",
        "#         except UnicodeDecodeError as e:\n",
        "#             st.error(f\"Error: {e}\")\n",
        "#             continue\n",
        "\n",
        "# def process_product_reviews(filtered_reviews, global_avg_rating, min_raters):\n",
        "#     if filtered_reviews.empty:\n",
        "#         return {\n",
        "#             \"mean_grade\": None,\n",
        "#             \"final_rate\": None,\n",
        "#             \"grades\": [],\n",
        "#             \"review_output\": []\n",
        "#         }\n",
        "\n",
        "#     grades, total_weighted_rating, total_mean_grade = [], 0, 0\n",
        "#     review_output = []\n",
        "\n",
        "#     for _, row in filtered_reviews.iterrows():\n",
        "#         review_text = preprocess_text(row[\"product_review_name\"])\n",
        "#         sentiment_label, grade = generate_review_grade_with_sentiment(review_text)\n",
        "\n",
        "#         if grade is not None:\n",
        "#             grades.append(grade)\n",
        "#             review_output.append({\n",
        "#                 \"review\": row['product_review_name'],\n",
        "#                 \"sentiment\": sentiment_label,\n",
        "#                 \"grade\": grade\n",
        "#             })\n",
        "\n",
        "#             weighted_rating = (\n",
        "#                 (row[\"product_rating\"] * row[\"product_number_of_rating\"])\n",
        "#                 + (global_avg_rating * min_raters)\n",
        "#             ) / (row[\"product_number_of_rating\"] + min_raters)\n",
        "#             total_weighted_rating += weighted_rating\n",
        "#             total_mean_grade += grade\n",
        "\n",
        "#     if grades:\n",
        "#         mean_grade = sum(grades) / len(grades)\n",
        "#         final_rate = (total_mean_grade / len(grades) + total_weighted_rating / len(filtered_reviews)) / 2\n",
        "#         return {\n",
        "#             \"mean_grade\": mean_grade,\n",
        "#             \"final_rate\": final_rate,\n",
        "#             \"grades\": grades,\n",
        "#             \"review_output\": review_output\n",
        "#         }\n",
        "#     else:\n",
        "#         return {\n",
        "#             \"mean_grade\": None,\n",
        "#             \"final_rate\": None,\n",
        "#             \"grades\": [],\n",
        "#             \"review_output\": []\n",
        "#         }\n",
        "\n",
        "# # Streamlit App Layout\n",
        "# st.title(\"Product Review Analyzer and Grader\")\n",
        "\n",
        "# # Input product name for summarization, pros/cons extraction, and grading\n",
        "# product_name = st.text_input(\"Enter Product Name:\")\n",
        "\n",
        "# if product_name:\n",
        "#     default_encoding = \"latin1\"\n",
        "#     st.subheader(f\"Reviews for {product_name}\")\n",
        "#     try:\n",
        "#         df = pd.read_csv(\"/content/English_Reviews_WithNewDateISO&IDColumn-WhichIdon'tAgreeOn.csv\", encoding=default_encoding)\n",
        "#     except UnicodeDecodeError as e:\n",
        "#         st.error(f\"Error reading file: {e}\")\n",
        "\n",
        "#     filtered_reviews = df[df[\"product_name\"].str.contains(product_name, case=False)]\n",
        "\n",
        "#     if not filtered_reviews.empty:\n",
        "#         # Sort by Date and get the latest 5 reviews\n",
        "#         filtered_reviews['Date'] = pd.to_datetime(filtered_reviews['Date'])\n",
        "#         latest_reviews = filtered_reviews.sort_values(by='Date', ascending=False).head(5)\n",
        "\n",
        "#         combined_reviews_text = \" \".join(filtered_reviews[\"product_review_name\"].tolist())\n",
        "\n",
        "#         # Summarize reviews\n",
        "#         st.subheader(\"Summarization\")\n",
        "#         summary = generate_summary(combined_reviews_text)\n",
        "#         st.write(f\"Summary:\\n{summary}\")\n",
        "\n",
        "#         # Generate pros and cons\n",
        "#         st.subheader(\"Pros and Cons\")\n",
        "#         pros, cons = generate_pros_and_cons(combined_reviews_text)\n",
        "#         st.write(f\"**Pros:**\\n{pros}\")\n",
        "#         st.write(f\"**Cons:**\\n{cons}\")\n",
        "\n",
        "#         # Calculate grades\n",
        "#         st.subheader(\"Grades and Ratings\")\n",
        "#         result = process_product_reviews(filtered_reviews, df[\"product_rating\"].mean(), 35)\n",
        "\n",
        "#         # Display the reviews with sentiment and grade\n",
        "#         for review_info in result[\"review_output\"]:\n",
        "#             st.write(f\"Review: {review_info['review']}\")\n",
        "#             st.write(f\"Sentiment: {review_info['sentiment']}\")\n",
        "#             st.write(f\"Grade: {review_info['grade']}\")\n",
        "\n",
        "\n",
        "#         # Show all grades in JSON format\n",
        "#         all_grades_json = {\n",
        "#             \"product_name\": product_name,\n",
        "#             \"grades\": result[\"grades\"],\n",
        "#             \"mean_grade\": result[\"mean_grade\"],\n",
        "#             \"final_rate\": result[\"final_rate\"],\n",
        "#         }\n",
        "#         st.json(all_grades_json)\n",
        "\n",
        "#     else:\n",
        "#         st.error(f\"No reviews found for product: {product_name}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYknyz2aHzAJ",
        "outputId": "61c51f4f-130f-49c5-d838-81077de705b8"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Set your API key here directly\n",
        "os.environ[\"API_KEY\"] = \"AIzaSyAFHyRhWWEVGTzNXH3xHq8vBx229DzVkPM\"\n",
        "genai.configure(api_key=os.environ[\"API_KEY\"])\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "# Load schema for Gemini model (if needed for your specific summarization task)\n",
        "with open(\"./scheme.json\", \"r\") as f:\n",
        "    gemini_flash_schema = json.load(f)\n",
        "\n",
        "# Preprocess text function\n",
        "def preprocess_text(text):\n",
        "    stopwords = {\n",
        "        \"the\", \"is\", \"in\", \"at\", \"on\", \"a\", \"an\", \"and\", \"or\", \"for\", \"to\", \"of\", \"with\", \"that\", \"by\", \"it\",\n",
        "    }\n",
        "    text = re.sub(r\"\\d+|[^\\w\\s]|\\s+\", \" \", text.lower()).strip()\n",
        "    return \" \".join([word for word in text.split() if word not in stopwords])\n",
        "\n",
        "# Generate sentiment and grade using Gemini\n",
        "def generate_review_grade_with_sentiment(review_text):\n",
        "    try:\n",
        "        prompt = f\"\"\"\n",
        "        Analyze the following review: {review_text}.\n",
        "\n",
        "        Determine its sentiment (positive, neutral, or negative) based on your analysis. You can use these examples as a reference, but the actual sentiment should be based on the review's content:\n",
        "        - **Positive**: \"The product was exactly as described, high quality, and arrived quickly.\" (Example grade: 4 or 5)\n",
        "        - **Neutral**: \"The product is okay, nothing special, but it works as expected.\" (Example grade: 3)\n",
        "        - **Negative**: \"The product was poorly made, broke easily, and did not meet expectations.\" (Example grade: 1 or 2)\n",
        "\n",
        "        After analyzing the review, assign a grade from 1 to 5:\n",
        "        - **4 or 5** for positive reviews.\n",
        "        - **3** for neutral reviews.\n",
        "        - **1 or 2** for negative reviews.\n",
        "\n",
        "        Make sure the grade reflects the overall tone and content of the review.\n",
        "        \"\"\"\n",
        "        response = model.generate_content(prompt)\n",
        "\n",
        "        # Extract only sentiment and grade\n",
        "        sentiment_match = re.search(r\"(positive|negative|neutral)\", response.text, re.IGNORECASE)\n",
        "        grade_match = re.search(r\"\\d(\\.\\d+)?\", response.text)\n",
        "\n",
        "        if sentiment_match and grade_match:\n",
        "            sentiment_label = sentiment_match.group().upper()\n",
        "            grade = float(grade_match.group())\n",
        "            return sentiment_label, grade\n",
        "        else:\n",
        "            return \"Unknown\", None\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error generating sentiment and grade for review: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Generate summary using Gemini\n",
        "def generate_summary(text):\n",
        "    try:\n",
        "        schema_str = json.dumps(gemini_flash_schema)\n",
        "        prompt = f\"Using the following constraints: {schema_str}, summarize the following text: {text}\"\n",
        "        response = model.generate_content(prompt)\n",
        "        summary = response.text.strip()\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error generating summary: {e}\")\n",
        "        return \"Summary could not be generated.\"\n",
        "\n",
        "# Generate pros and cons using Gemini\n",
        "def generate_pros_and_cons(text):\n",
        "    try:\n",
        "        schema_str = json.dumps(gemini_flash_schema)\n",
        "        prompt = f\"Using the following constraints: {schema_str}, extract pros and cons from the following text: {text}\"\n",
        "        response = model.generate_content(prompt)\n",
        "        response_text = response.text.strip()\n",
        "\n",
        "        pros, cons = \"\", \"\"\n",
        "        if \"Pros:\" in response_text:\n",
        "            pros = response_text.split(\"Pros:\")[1].split(\"Cons:\")[0].strip()\n",
        "        if \"Cons:\" in response_text:\n",
        "            cons = response_text.split(\"Cons:\")[1].strip()\n",
        "\n",
        "        return pros, cons\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error generating pros and cons: {e}\")\n",
        "        return \"Pros could not be generated.\", \"Cons could not be generated.\"\n",
        "\n",
        "# Calculate mean grades\n",
        "def calculate_mean_grades():\n",
        "    encodings = [\"latin1\", \"ISO-8859-1\", \"cp1252\"]\n",
        "    for enc in encodings:\n",
        "        try:\n",
        "            df = pd.read_csv(\"/content/English_Reviews_WithNewDateISO&IDColumn-WhichIdon'tAgreeOn.csv\", encoding=enc)\n",
        "            global_avg_rating = df[\"product_rating\"].mean()\n",
        "            min_raters = 35\n",
        "            result = {}\n",
        "\n",
        "            for product in df[\"product_name\"].unique():\n",
        "                filtered_reviews = df[df[\"product_name\"] == product]\n",
        "                result[product] = process_product_reviews(filtered_reviews, global_avg_rating, min_raters)\n",
        "\n",
        "            st.write(\"Product Grades:\")\n",
        "            st.json(result)\n",
        "            return result\n",
        "\n",
        "        except UnicodeDecodeError as e:\n",
        "            st.error(f\"Error: {e}\")\n",
        "            continue\n",
        "\n",
        "def process_product_reviews(filtered_reviews, global_avg_rating, min_raters):\n",
        "    if filtered_reviews.empty:\n",
        "        return {\n",
        "            \"mean_grade\": None,\n",
        "            \"final_rate\": None,\n",
        "            \"grades\": [],\n",
        "            \"review_output\": []\n",
        "        }\n",
        "\n",
        "    grades, total_weighted_rating, total_mean_grade = [], 0, 0\n",
        "    review_output = []\n",
        "\n",
        "    for _, row in filtered_reviews.iterrows():\n",
        "        review_text = preprocess_text(row[\"product_review_name\"])\n",
        "        sentiment_label, grade = generate_review_grade_with_sentiment(review_text)\n",
        "\n",
        "        if grade is not None:\n",
        "            grades.append(grade)\n",
        "            review_output.append({\n",
        "                \"review\": row['product_review_name'],\n",
        "                \"sentiment\": sentiment_label,\n",
        "                \"grade\": grade\n",
        "            })\n",
        "\n",
        "            weighted_rating = (\n",
        "                (row[\"product_rating\"] * row[\"product_number_of_rating\"])\n",
        "                + (global_avg_rating * min_raters)\n",
        "            ) / (row[\"product_number_of_rating\"] + min_raters)\n",
        "            total_weighted_rating += weighted_rating\n",
        "            total_mean_grade += grade\n",
        "\n",
        "    if grades:\n",
        "        mean_grade = sum(grades) / len(grades)\n",
        "        final_rate = (total_mean_grade / len(grades) + total_weighted_rating / len(filtered_reviews)) / 2\n",
        "        return {\n",
        "            \"mean_grade\": mean_grade,\n",
        "            \"final_rate\": final_rate,\n",
        "            \"grades\": grades,\n",
        "            \"review_output\": review_output\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"mean_grade\": None,\n",
        "            \"final_rate\": None,\n",
        "            \"grades\": [],\n",
        "            \"review_output\": []\n",
        "        }\n",
        "\n",
        "# Streamlit App Layout\n",
        "st.title(\"Product Review Analyzer and Grader\")\n",
        "\n",
        "# Input product name for summarization, pros/cons extraction, and grading\n",
        "product_name = st.text_input(\"Enter Product Name:\")\n",
        "\n",
        "if product_name:\n",
        "    default_encoding = \"latin1\"\n",
        "    st.subheader(f\"Reviews for {product_name}\")\n",
        "    try:\n",
        "        df = pd.read_csv(\"/content/English_Reviews_WithNewDateISO&IDColumn-WhichIdon'tAgreeOn.csv\", encoding=default_encoding)\n",
        "    except UnicodeDecodeError as e:\n",
        "        st.error(f\"Error reading file: {e}\")\n",
        "\n",
        "    filtered_reviews = df[df[\"product_name\"].str.contains(product_name, case=False)]\n",
        "\n",
        "    if not filtered_reviews.empty:\n",
        "        # Sort by Date and get the latest 5 reviews\n",
        "        filtered_reviews['Date'] = pd.to_datetime(filtered_reviews['Date'])\n",
        "        latest_reviews = filtered_reviews.sort_values(by='Date', ascending=False).head(5)\n",
        "\n",
        "        combined_reviews_text = \" \".join(filtered_reviews[\"product_review_name\"].tolist())\n",
        "\n",
        "        # Summarize reviews\n",
        "        st.subheader(\"Summarization\")\n",
        "        summary = generate_summary(combined_reviews_text)\n",
        "        st.write(f\"Summary:\\n{summary}\")\n",
        "\n",
        "        # Generate pros and cons\n",
        "        st.subheader(\"Pros and Cons\")\n",
        "        pros, cons = generate_pros_and_cons(combined_reviews_text)\n",
        "        st.write(f\"**Pros:**\\n{pros}\")\n",
        "        st.write(f\"**Cons:**\\n{cons}\")\n",
        "\n",
        "        # Calculate grades only for the latest 5 reviews\n",
        "        st.subheader(\"Grades and Ratings for Latest Reviews\")\n",
        "        latest_result = process_product_reviews(latest_reviews, df[\"product_rating\"].mean(), 35)\n",
        "\n",
        "        # Display the latest reviews with sentiment and grade\n",
        "        for review_info in latest_result[\"review_output\"]:\n",
        "            st.write(f\"Review: {review_info['review']}\")\n",
        "            st.write(f\"Sentiment: {review_info['sentiment']}\")\n",
        "            st.write(f\"Grade: {review_info['grade']}\")\n",
        "\n",
        "        # Calculate overall grades for all reviews\n",
        "        overall_result = process_product_reviews(filtered_reviews, df[\"product_rating\"].mean(), 35)\n",
        "\n",
        "        # Show all grades in JSON format\n",
        "        all_grades_json = {\n",
        "            \"product_name\": product_name,\n",
        "            \"grades\": overall_result[\"grades\"],\n",
        "            \"mean_grade\": overall_result[\"mean_grade\"],\n",
        "            \"final_rate\": overall_result[\"final_rate\"],\n",
        "        }\n",
        "        st.json(all_grades_json)\n",
        "\n",
        "    else:\n",
        "        st.error(f\"No reviews found for product: {product_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfBEVoBlKJpb",
        "outputId": "3d9ac526-8495-4283-818e-5e8b4da9494e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Replace with your ngrok auth token\n",
        "ngrok.set_auth_token('2ltdr1eT1IHKxEj4GCQvhFb9jkU_4y5ggmbqPQwVoMvWjy1EZ')\n",
        "\n",
        "# Open a ngrok tunnel to the Streamlit port 8502\n",
        "public_url = ngrok.connect(8502)\n",
        "print('Streamlit app is live at:', public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sC5UoHB2ryP",
        "outputId": "f0c7ac89-e30d-4ddc-84a3-745f6c3a057e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is live at: NgrokTunnel: \"https://d27e-34-80-72-80.ngrok-free.app\" -> \"http://localhost:8502\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port 8502 &\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xN-sbyo3Cgu",
        "outputId": "af19b377-d078-4769-a043-ac209eec08a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8502\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8502\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.80.72.80:8502\u001b[0m\n",
            "\u001b[0m\n",
            "/content/app.py:178: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_reviews['Date'] = pd.to_datetime(filtered_reviews['Date'])\n"
          ]
        }
      ]
    }
  ]
}